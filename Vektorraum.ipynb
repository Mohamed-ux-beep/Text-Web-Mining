{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In der folgenden Aufgabe soll die praktische Umsetzung von Dokumentvektoren mit Python und scikit-learn trainiert werden! Verwenden Sie in dieser Aufgabe den von Ihnen in der letzten Woche gewählten Datensatz. Korrigieren Sie ggf. vorher noch einmal die Textvorverarbeitung entsprechend der in der Übung besprochenen Probleme!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "from nltk.stem import PorterStemmer\n",
    "import pprint \n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pprint\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Origin/Ethnicity</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Wiki Page</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1901</td>\n",
       "      <td>Kansas Saloon Smashers</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1901</td>\n",
       "      <td>Love by the Light of the Moon</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1901</td>\n",
       "      <td>The Martyred Presidents</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1901</td>\n",
       "      <td>Terrible Teddy, the Grizzly King</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1902</td>\n",
       "      <td>Jack and the Beanstalk</td>\n",
       "      <td>American</td>\n",
       "      <td>George S. Fleming, Edwin S. Porter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Release Year                             Title Origin/Ethnicity  \\\n",
       "0          1901            Kansas Saloon Smashers         American   \n",
       "1          1901     Love by the Light of the Moon         American   \n",
       "2          1901           The Martyred Presidents         American   \n",
       "3          1901  Terrible Teddy, the Grizzly King         American   \n",
       "4          1902            Jack and the Beanstalk         American   \n",
       "\n",
       "                             Director Cast    Genre  \\\n",
       "0                             Unknown  NaN  unknown   \n",
       "1                             Unknown  NaN  unknown   \n",
       "2                             Unknown  NaN  unknown   \n",
       "3                             Unknown  NaN  unknown   \n",
       "4  George S. Fleming, Edwin S. Porter  NaN  unknown   \n",
       "\n",
       "                                           Wiki Page  \\\n",
       "0  https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
       "1  https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
       "2  https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
       "3  https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
       "4  https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
       "\n",
       "                                                Plot  \n",
       "0  A bartender is working at a saloon, serving dr...  \n",
       "1  The moon, painted with a smiling face hangs ov...  \n",
       "2  The film, just over a minute long, is composed...  \n",
       "3  Lasting just 61 seconds and consisting of two ...  \n",
       "4  The earliest known adaptation of the classic f...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lesen des Datensatzes\n",
    "df = pd.read_csv('wiki_movie_plots_deduped.csv')\n",
    "\n",
    "# Zeig die ersten 5 Zeile \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten, für sie wir uns interessieren \n",
    "text_data = df['Plot'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_nltk_tokenizer(doc, condition=False):\n",
    "\n",
    "    # stopwörter \n",
    "    stopwörter = list(set(stopwords.words('english')))\n",
    "    \n",
    "    # Stemmer \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    # tokenisierung\n",
    "    tokens_lis = word_tokenize(doc, language='english')\n",
    "    if condition:\n",
    "        print(tokens_lis[:5])\n",
    "    \n",
    "    # filterierung \n",
    "    tokens_lis = [element.lower() for element in tokens_lis if element.isalnum()]\n",
    "    if condition:\n",
    "        print(tokens_lis[:5])\n",
    "    \n",
    "    # stopwörter entfernen \n",
    "    tokens_lis = [element for element in tokens_lis if element not in stopwörter]\n",
    "    if condition:\n",
    "        print(tokens_lis[:5])\n",
    "    \n",
    "    # stemming \n",
    "    tokens_lis = [ps.stem(element) for element in tokens_lis]\n",
    "    if condition:\n",
    "        print(tokens_lis[:5])\n",
    "    \n",
    "    return  tokens_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'bartender', 'is', 'working', 'at']\n",
      "['a', 'bartender', 'is', 'working', 'at']\n",
      "['bartender', 'working', 'saloon', 'serving', 'drinks']\n",
      "['bartend', 'work', 'saloon', 'serv', 'drink']\n"
     ]
    }
   ],
   "source": [
    "# join the data\n",
    "doc = \" \".join(text_data)\n",
    "tokens_lis = my_nltk_tokenizer(doc, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bestimmen Sie das Wörterbuch mit scikit-learn und geben Sie es aus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_count = CountVectorizer(tokenizer=my_nltk_tokenizer, encoding=u'utf-8')\n",
    "vecs_count = vectorizer_count.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vecs_count.pkl', 'wb') as f:\n",
    "    pickle.dump(vecs_count, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Größe des Worterbuchs:  102249\n",
      "['0' '00' '000' ... '초록' '초록머리' 'ﬂight']\n"
     ]
    }
   ],
   "source": [
    "print(\"Größe des Worterbuchs: \", str(len(vectorizer_count.get_feature_names_out())))\n",
    "print(vectorizer_count.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer_count.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bestimmen Sie den Häufigkeitsvektor und den TF-IDF-Vektor für das erste Dokument mit scikit-learn und geben Sie diese jeweils aus! Probieren Sie verschiedene Gewichtungsschema durch unterschiedliche Parameterwerte! Geben Sie die resultierenden Vektoren aus und vergleichen Sie sie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Häufigkeitsvektoren\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Häufigkeitsvektoren\")\n",
    "print(vecs_count.toarray()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf \n",
    "vectorizer_tfidf = TfidfVectorizer(tokenizer=my_nltk_tokenizer)\n",
    "vecs_tfidf = vectorizer_tfidf.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vecs_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(vecs_tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Größe des Worterbuchs:  102249\n",
      "['0' '00' '000' ... '초록' '초록머리' 'ﬂight']\n"
     ]
    }
   ],
   "source": [
    "print(\"Größe des Worterbuchs: \", str(len(vectorizer_tfidf.get_feature_names_out())))\n",
    "print(vectorizer_tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ = vectorizer_tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Häufigkeitsvektoren\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Häufigkeitsvektoren\")\n",
    "print(vecs_tfidf.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_count[vecs_count != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.10993181, 0.07864447, 0.11735617, ..., 0.1497787 , 0.0747065 ,\n",
       "         0.31778998]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_tfidf[vecs_tfidf != 0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11b2272c4f21ba7ef91cd620dbfd5e8689933469eea99b00bd1c0ddf5328b9ff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11b2272c4f21ba7ef91cd620dbfd5e8689933469eea99b00bd1c0ddf5328b9ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
