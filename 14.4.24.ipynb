{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mall libraries are imported\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import SnowballStemmer\n",
    "import nltk\n",
    "print('\\033[33mall libraries are imported\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Daten Einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wiki_movie_plots_deduped.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Datenframe erstellen um die Ergebnisse abspeichern zu können "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['10 Dok', '100 Dok', '1000 Dok', 'Gesamte Sammlung']\n",
    "ind = ['Nur Tokenisierung', 'Plus Stopwordentfernung', 'Plus Stemming']\n",
    "results_df = pd.DataFrame(columns=cols, index=ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Origin/Ethnicity</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Wiki Page</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1901</td>\n",
       "      <td>Kansas Saloon Smashers</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1901</td>\n",
       "      <td>Love by the Light of the Moon</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1901</td>\n",
       "      <td>The Martyred Presidents</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1901</td>\n",
       "      <td>Terrible Teddy, the Grizzly King</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1902</td>\n",
       "      <td>Jack and the Beanstalk</td>\n",
       "      <td>American</td>\n",
       "      <td>George S. Fleming, Edwin S. Porter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Release Year                             Title Origin/Ethnicity  \\\n",
       "0          1901            Kansas Saloon Smashers         American   \n",
       "1          1901     Love by the Light of the Moon         American   \n",
       "2          1901           The Martyred Presidents         American   \n",
       "3          1901  Terrible Teddy, the Grizzly King         American   \n",
       "4          1902            Jack and the Beanstalk         American   \n",
       "\n",
       "                             Director Cast    Genre  \\\n",
       "0                             Unknown  NaN  unknown   \n",
       "1                             Unknown  NaN  unknown   \n",
       "2                             Unknown  NaN  unknown   \n",
       "3                             Unknown  NaN  unknown   \n",
       "4  George S. Fleming, Edwin S. Porter  NaN  unknown   \n",
       "\n",
       "                                           Wiki Page  \\\n",
       "0  https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
       "1  https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
       "2  https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
       "3  https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
       "4  https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
       "\n",
       "                                                Plot  \n",
       "0  A bartender is working at a saloon, serving dr...  \n",
       "1  The moon, painted with a smiling face hangs ov...  \n",
       "2  The film, just over a minute long, is composed...  \n",
       "3  Lasting just 61 seconds and consisting of two ...  \n",
       "4  The earliest known adaptation of the classic f...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['Plot']\n",
    "data = data.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lexikalische Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [word_tokenize(sentence) for sentence in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Füllen das DatenFrame mit tokenisierten Dokumenten\n",
    "results_df['10 Dok']['Nur Tokenisierung'] = tokenized_data[:10]\n",
    "results_df['100 Dok']['Nur Tokenisierung'] = tokenized_data[:100]\n",
    "results_df['1000 Dok']['Nur Tokenisierung'] = tokenized_data[:1000]\n",
    "results_df['Gesamte Sammlung']['Nur Tokenisierung'] = tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stops Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mwir haben 179 stop words in Englischer Sprache ..\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = list(stop_words)\n",
    "print(f\"\\033[33mwir haben {len(stop_words)} stop words in Englischer Sprache ..\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mStop words und Spezifische Charakter und Zahlen sind schon entfernt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenized_data_ = []\n",
    "for sublist in tokenized_data:\n",
    "    rest = [element for element in sublist if element.lower() not in stop_words]\n",
    "    rest = [element for element in rest if element not in string.punctuation]\n",
    "    rest = [element for element in rest if not re.match(r'^\\d+$|\\d+\\-\\d+', element)]\n",
    "    rest = [token for token in rest if token not in [\"``\", \"''\"]]\n",
    "    rest = [token.split('—')[0] for token in rest]\n",
    "    rest = [token.split('-')[0] for token in rest]\n",
    "    tokenized_data_.append(rest)\n",
    "\n",
    "print(\"\\033[33mStop words und Spezifische Charakter und Zahlen sind schon entfernt\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['10 Dok']['Plus Stopwordentfernung'] = tokenized_data_[:10]\n",
    "results_df['100 Dok']['Plus Stopwordentfernung'] = tokenized_data_[:100]\n",
    "results_df['1000 Dok']['Plus Stopwordentfernung'] = tokenized_data_[:1000]\n",
    "results_df['Gesamte Sammlung']['Plus Stopwordentfernung'] = tokenized_data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mStemming erledigt !\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "stemmed_data_ = []\n",
    "ps = PorterStemmer()\n",
    "for sublist in tokenized_data_:\n",
    "    rest = [ps.stem(token) for token in sublist]\n",
    "    stemmed_data_.append(rest)\n",
    "print(\"\\033[33mStemming erledigt !\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['10 Dok']['Plus Stemming'] = stemmed_data_[:10]\n",
    "results_df['100 Dok']['Plus Stemming'] = stemmed_data_[:100]\n",
    "results_df['1000 Dok']['Plus Stemming'] = stemmed_data_[:1000]\n",
    "results_df['Gesamte Sammlung']['Plus Stemming'] = stemmed_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 Dok</th>\n",
       "      <th>100 Dok</th>\n",
       "      <th>1000 Dok</th>\n",
       "      <th>Gesamte Sammlung</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nur Tokenisierung</th>\n",
       "      <td>[[A, bartender, is, working, at, a, saloon, ,,...</td>\n",
       "      <td>[[A, bartender, is, working, at, a, saloon, ,,...</td>\n",
       "      <td>[[A, bartender, is, working, at, a, saloon, ,,...</td>\n",
       "      <td>[[A, bartender, is, working, at, a, saloon, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plus Stopwordentfernung</th>\n",
       "      <td>[[bartender, working, saloon, serving, drinks,...</td>\n",
       "      <td>[[bartender, working, saloon, serving, drinks,...</td>\n",
       "      <td>[[bartender, working, saloon, serving, drinks,...</td>\n",
       "      <td>[[bartender, working, saloon, serving, drinks,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plus Stemming</th>\n",
       "      <td>[[bartend, work, saloon, serv, drink, custom, ...</td>\n",
       "      <td>[[bartend, work, saloon, serv, drink, custom, ...</td>\n",
       "      <td>[[bartend, work, saloon, serv, drink, custom, ...</td>\n",
       "      <td>[[bartend, work, saloon, serv, drink, custom, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    10 Dok  \\\n",
       "Nur Tokenisierung        [[A, bartender, is, working, at, a, saloon, ,,...   \n",
       "Plus Stopwordentfernung  [[bartender, working, saloon, serving, drinks,...   \n",
       "Plus Stemming            [[bartend, work, saloon, serv, drink, custom, ...   \n",
       "\n",
       "                                                                   100 Dok  \\\n",
       "Nur Tokenisierung        [[A, bartender, is, working, at, a, saloon, ,,...   \n",
       "Plus Stopwordentfernung  [[bartender, working, saloon, serving, drinks,...   \n",
       "Plus Stemming            [[bartend, work, saloon, serv, drink, custom, ...   \n",
       "\n",
       "                                                                  1000 Dok  \\\n",
       "Nur Tokenisierung        [[A, bartender, is, working, at, a, saloon, ,,...   \n",
       "Plus Stopwordentfernung  [[bartender, working, saloon, serving, drinks,...   \n",
       "Plus Stemming            [[bartend, work, saloon, serv, drink, custom, ...   \n",
       "\n",
       "                                                          Gesamte Sammlung  \n",
       "Nur Tokenisierung        [[A, bartender, is, working, at, a, saloon, ,,...  \n",
       "Plus Stopwordentfernung  [[bartender, working, saloon, serving, drinks,...  \n",
       "Plus Stemming            [[bartend, work, saloon, serv, drink, custom, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begrifflichkeiten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['masterstudium', 'voll', 'herausforder', 'den', 'natur', 'gern', 'stell']\n",
      " \n",
      "{'masterstudium': 1, 'voll': 1, 'herausforder': 1, 'den': 1, 'natur': 1, 'gern': 1, 'stell': 1}\n",
      " \n",
      "(S\n",
      "  masterstudium/NN\n",
      "  voll/NN\n",
      "  herausforder/NN\n",
      "  den/NN\n",
      "  natur/NN\n",
      "  gern/NN\n",
      "  stell/NN)\n"
     ]
    }
   ],
   "source": [
    "sent = \"Das Masterstudium ist voller Herausforderungen, denen wir uns aber natürlich gerne stellen\"\n",
    "tokens = word_tokenize(sent.lower())\n",
    "psg = SnowballStemmer('german')\n",
    "\n",
    "# stop word removal \n",
    "tokens = [token for token in tokens if token not in list(stopwords.words('german'))]\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "tokens = [psg.stem(token) for token in tokens]\n",
    "\n",
    "print(tokens)\n",
    "print(\" \")\n",
    "\n",
    "# bag of words \n",
    "token_dic_n = {}\n",
    "\n",
    "for tok in tokens:\n",
    "    token_dic_n[tok] = token_dic_n.get(tok, 0) + 1\n",
    "    \n",
    "print(token_dic_n)\n",
    "print(\" \")\n",
    "# semantische Analyse \n",
    "tokens = nltk.pos_tag(tokens)\n",
    "tree = nltk.ne_chunk(tokens, binary=False)\n",
    "\n",
    "print(tree)\n",
    "\n",
    "tree.draw()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
