{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from nltk.probability import FreqDist\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *TransformerMixin* : assure we have fit_transform and we can write\n",
    " \n",
    "* *BaseEstimator*    : assure that we have get_params and set_params for free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Transformer, extracts column passed as arg to constructor\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    # return just self \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    # describe what we need this transformer to do \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.feature_names].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing of the data \n",
    "class Preprocess_text(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, raw_data, ps):\n",
    "        self.raw_data = raw_data\n",
    "        self.ps = PortStemmer()\n",
    " \n",
    "    # tokenize the data \n",
    "    def tokenize(self):\n",
    "        self.tokenized_data = [word_tokenize(item) for item in raw_data]\n",
    "        self.tokenized_data = [item for sublist in tokenized_data for item in sublist]\n",
    "        return self.tokenized_data\n",
    "\n",
    "    # get only alphabetic and numerical elements \n",
    "    def text_data(self):\n",
    "        self.raw_data = [item.lower() for item in raw_data if item.isalnum()]\n",
    "        return self.raw_data\n",
    "\n",
    "    # remove stop words \n",
    "    def remove_sw(self):\n",
    "        self.tokenized_data = [item for item in tokenized_data if item not in set(stopwords.words('english'))]\n",
    "\n",
    "    # stemming\n",
    "    def stemming(self):\n",
    "        self.stemmed_data = [ps.stem(item) for item in self.tokenized_data]\n",
    "        return stemmed_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mohamed_transformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, raw_data):\n",
    "        self.raw_data = [item.lower() for item in raw_data]\n",
    "        self.tokens = [item.lower() for item in word_tokenize(\" \".join(raw_data))]\n",
    "        self.lenn = len(raw_data) # total number of documents \n",
    "        self.df = pd.DataFrame(columns=[f'doc {i}' for i in range(len(self.raw_data))], index=list(set([token.lower() for token in self.tokens])))\n",
    "        self.norm_df = pd.DataFrame(columns=[f'doc {i}' for i in range(len(self.raw_data))], index=['max norm'])\n",
    "\n",
    "    # get the dictionary\n",
    "    def make_dictionary(self):\n",
    "        self.dictionary = {text: i for i,text in enumerate(self.raw_data)}\n",
    "        return self.dictionary\n",
    "\n",
    "    def inverse_indexing(self):\n",
    "        self.indexing = {key.lower(): [] for i, key in enumerate(self.tokens)}\n",
    "        for _ in range(len(self.tokens)):\n",
    "            # if token exists in doc\n",
    "            token = self.tokens[_]\n",
    "            for i in range(len(self.raw_data)):\n",
    "\n",
    "                if token in self.raw_data[i]:\n",
    "                    if self.dictionary[self.raw_data[i]] not in self.indexing[token]:\n",
    "                        self.indexing[token].append(self.dictionary[self.raw_data[i]])\n",
    "        return self.indexing\n",
    "\n",
    "    def log_tf(self):\n",
    "        self.df[:] = 0\n",
    "        for token in self.tokens:\n",
    "\n",
    "            for i, text in enumerate(self.raw_data):\n",
    "                freq_dis = FreqDist(text)\n",
    "\n",
    "                if token in freq_dis.keys():\n",
    "                    self.df[f\"doc {i}\"][token] += freq_dis[token]\n",
    "                else:\n",
    "                    self.df[f\"doc {i}\"][token] = 1\n",
    "        for col in self.df:\n",
    "            self.df[col] = self.df[col].apply(lambda x: 1 + math.log(x))\n",
    "            \n",
    "        self.df = self.df.apply(pd.to_numeric, errors='coerce')\n",
    "        return self.df\n",
    "\n",
    "    def prob_idf(self):\n",
    "        self.dictionary_idf = {key: max(0, math.log((self.lenn-len(self.indexing[key])/len(self.indexing[key])))) for i, key in enumerate(self.tokens)}\n",
    "        return self.dictionary_idf\n",
    "\n",
    "    def get_norm(self):\n",
    "        for col in self.df.columns:\n",
    "            value = max(self.df[col])\n",
    "            self.norm_df[col]['max norm'] = value\n",
    "\n",
    "        return self.norm_df\n",
    "\n",
    "    # tf * idf / norm for each token\n",
    "    def result(self): \n",
    "        self.res = []\n",
    "        for text in self.raw_data:\n",
    "            tokenized = word_tokenize(text)\n",
    "            x = []\n",
    "            for i in range(len(tokenized)):\n",
    "                tf_i = self.df[f'doc {self.dictionary[text]}'][tokenized[i]] \n",
    "                idf_i = self.dictionary_idf[tokenized[i]]\n",
    "                norm_i = self.norm_df[f\"doc {self.dictionary[text]}\"]['max norm']\n",
    "                x.append(round(tf_i * idf_i / norm_i,2))\n",
    "            self.res.append(x)\n",
    "        return self.res  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"Actions speak louder than words.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.69],\n",
       " [0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.69],\n",
       " [0.33, 0.33, 0.33, 0.33, 0.33, 0.69]]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = mohamed_transformer(sentences)\n",
    "instance.make_dictionary()\n",
    "instance.inverse_indexing()\n",
    "instance.log_tf()\n",
    "instance.prob_idf()\n",
    "instance.get_norm()\n",
    "instance.result()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11b2272c4f21ba7ef91cd620dbfd5e8689933469eea99b00bd1c0ddf5328b9ff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
