{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook zum Foliensatz Suche - Invertierter Index**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufbau des Datensatzes (Rohtexte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pprint\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus_text = [] # Der zunächst leere Korpus -> Liste mit Texten\n",
    "for filename in os.listdir('TuWM02data'):\n",
    "    f = open('TuWM02data/'+filename)\n",
    "    corpus_text.append(f.read())\n",
    "\n",
    "#pprint.pprint(corpus_text,width=180,compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vektorisieren**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Größe des Wörterbuchs: 220\n",
      "IDs der Token\n",
      "{'neu': 156, 'bibliothek': 55, 'standort': 179, 'dessau': 64, 'seit': 176, 'somm': 178, '2009': 5, 'konnt': 133, 'umbauarbeit': 195, 'ehemal': 68, 'kaufhall': 126, 'bauhaus': 35, 'gemeinsam': 103, 'hochschul': 112, 'anhalt': 19, 'stiftung': 183, 'verfolg': 203, 'marz': 149, '2012': 6, 'erfolgt': 85, 'feierlich': 95, 'einweih': 76, 'bereit': 47, 'kurz': 138, 'grundung': 107, 'hochschulstandort': 116, '1993': 1, 'befand': 39, 'beid': 43, 'eher': 69, 'behelfsmass': 42, 'dach': 60, 'damal': 61, 'geschichtstracht': 104, 'rasant': 168, 'wachsend': 207, 'studierendenzahl': 186, 'ebenso': 67, 'bestand': 51, 'buch': 57, 'macht': 148, '1997': 2, 'auszug': 33, 'hochschulbibliothek': 113, 'jahnstrass': 122, 'unausweich': 199, 'lang': 142, 'such': 187, 'akzeptabl': 18, 'losung': 146, 'begann': 41, 'erst': 90, 'schritt': 174, 'ergab': 86, '2003': 3, 'erneut': 89, 'kooperation': 136, 'zwei': 219, 'eigenstand': 71, '2004': 4, 'wurd': 216, 'plan': 161, 'umnutz': 197, 'leer': 144, 'stehend': 182, 'gebaud': 101, 'tanzcafés': 188, 'verfolgt': 204, 'federfuhr': 94, 'landesbetrieb': 141, 'bau': 34, 'niederlass': 157, 'ost': 159, 'eng': 78, 'zusammenarbeit': 218, 'galt': 99, 'umgestalt': 196, 'modern': 153, 'umzusetz': 198, 'realisiert': 169, 'buro': 58, 'rein': 172, 'beck': 38, 'architekt': 23, 'bda': 36, 'ausfuhr': 26, 'innenausstatt': 119, 'deutsch': 65, 'werkstatt': 210, 'gewonn': 106, 'wintersem': 213, 'steh': 181, 'nutz': 158, 'gut': 109, 'gestaltet': 105, 'freundlich': 98, 'hell': 111, 'rechercheplatz': 170, 'verfug': 205, 'technologietransf': 189, 'transfergutschein': 192, 'einfuhr': 73, 'ministerium': 150, 'wissenschaft': 215, 'wirtschaft': 214, 'land': 140, 'insbesond': 120, 'kontinui': 135, 'bilateral': 56, 'austausch': 30, 'forschung': 97, 'betrieb': 54, 'entwickl': 80, 'klein': 127, 'mittelstand': 151, 'unternehm': 200, 'unterstutz': 202, 'gutschein': 110, 'geb': 100, 'partn': 160, 'geleg': 102, 'start': 180, 'auszubau': 32, 'dabei': 59, 'ziel': 217, 'studier': 185, 'erhalt': 88, 'moglich': 154, 'kontakt': 134, 'regional': 171, 'aufzunehm': 25, 'beruf': 49, 'erfahr': 83, 'konkret': 132, 'praxisaufgab': 163, 'sammeln': 173, 'beispielsweis': 45, 'bearbeit': 37, 'projekt': 166, 'praktikumsaufgab': 162, 'aufgabenstell': 24, 'abschlussarbeit': 15, 'wiederum': 212, 'besteht': 52, 'aktuell': 17, 'know': 128, 'how': 117, 'einfliess': 72, 'lass': 143, 'wert': 211, '400': 10, 'euro': 91, 'soll': 177, 'komm': 129, 'mittl': 152, 'eig': 70, 'entwicklungsabteil': 81, 'fachbereich': 92, 'insgesamt': 121, '84': 13, 'ausgeb': 28, 'april': 22, 'entsprech': 79, 'antrag': 21, 'betreu': 53, 'hochschullehrinn': 115, 'hochschullehr': 114, 'beim': 44, 'prodekan': 165, 'jeweil': 123, 'einzureich': 77, 'kommission': 130, 'pruf': 167, 'ausgab': 27, 'der': 63, 'gultig': 108, 'sech': 175, 'monat': 155, 'beschrankt': 50, 'vornehm': 206, 'abschluss': 14, 'eingereicht': 74, 'transferzentrum': 193, 'absolventenvermittl': 16, 'weiterbild': 208, 'beleg': 46, 'erbracht': 82, 'leistung': 145, 'komplett': 131, 'ausgefullt': 29, 'einschliess': 75, 'titel': 191, 'kurzbeschreib': 139, 'problem': 164, 'losungsweg': 147, 'dokumentation': 66, 'ergebnis': 87, 'unterschrift': 201, 'ubergeb': 194, 'danach': 62, 'auszahl': 31, 'erfolg': 84, 'weiterfuhr': 209, 'information': 118, 'befind': 40, 'flyer': 96, 'ansprechpartnerin': 20, 'katrin': 125, 'kaftan': 124, 'bernburg': 48, 'strass': 184, '55': 11, '06366': 0, 'koth': 137, 'telefon': 190, '3496': 9, '67': 12, '2315': 7, 'fax': 93, '2499': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff/bade_k/.local/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Tokenizierung wie beim letzten Mal als eigene Funktion zur Integration im Vectorizer\n",
    "def my_nltk_tokenizer(doc):\n",
    "    stemmer = SnowballStemmer('german')\n",
    "    stop_words = set(stopwords.words('german')) \n",
    "    tokens = word_tokenize(doc,language='german')   #Tokenisierung\n",
    "    tokens = [x.lower() for x in tokens if x.isalnum()]   #Kleinschreibung und Alphanumerische Zeichen\n",
    "    tokens = [x for x in tokens if not x in stop_words]   #Stoppwortentfernung\n",
    "    tokens = [stemmer.stem(x) for x in tokens]   #Wortstämme\n",
    "    return tokens\n",
    "\n",
    "## Vectorizer initialisieren, Wörterbuch aufbauen und Vektoren gleich umwandeln\n",
    "vectorizer_count = CountVectorizer(tokenizer=my_nltk_tokenizer)\n",
    "vecs_count = vectorizer_count.fit_transform(corpus_text)\n",
    "print(\"Größe des Wörterbuchs: \"+str(len(vectorizer_count.get_feature_names_out())))\n",
    "print(\"IDs der Token\")\n",
    "print(vectorizer_count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Häufigkeitsvektoren -> eigentlich Dokument-Term-Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 3 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 7\n",
      "  1 0 1 1 0 1 2 1 0 0 0 1 0 0 0 1 0 0 0 9 0 1 1 0 2 1 0 0 6 1 0 1 2 1 0 1\n",
      "  0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 2 0 2 1 1 1 1\n",
      "  0 1 0 1 3 1 0 0 1 0 0 1 0 0 1 0 0 0 2 0 0 0 0 0 0 2 0 0 1 0 1 0 0 1 1 0\n",
      "  1 0 1 0 1 1 0 0 0 2 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 4 0 1 1\n",
      "  0 1 1 3 0 0 1 1 1 0 0 0 0 0 0 1 2 1 1 1 0 0 0 1 1 1 0 2 0 0 1 0 0 1 0 0\n",
      "  1 0 1 1]\n",
      " [1 0 0 0 0 0 0 1 1 2 1 1 2 1 1 1 2 1 0 2 1 2 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
      "  0 2 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 2 2 0 1 0 0 2 0 0 1 1 0 0 1 0 0 0 1 0\n",
      "  1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 2 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0\n",
      "  1 1 3 0 3 0 1 1 0 1 1 0 2 1 0 1 1 1 0 2 1 1 1 1 1 0 1 1 1 1 0 1 3 0 0 1\n",
      "  0 1 1 1 0 0 2 1 1 0 2 1 0 0 0 0 1 0 1 1 1 1 4 1 0 0 0 1 0 1 0 1 0 1 0 0\n",
      "  1 0 0 0 1 3 0 0 0 1 1 1 5 2 1 0 0 0 0 0 4 1 1 0 1 0 1 0 2 1 0 1 1 0 3 5\n",
      "  0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#print(vecs_count)\n",
    "#print(\" \")\n",
    "print(vecs_count.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Die Dokument-Term-Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix-Typ: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Matrix-Dimensionen:  (2, 220)\n",
      "\n",
      "Ein Dokumentvektor\n",
      "  (0, 156)\t1\n",
      "  (0, 55)\t9\n",
      "  (0, 179)\t1\n",
      "  (0, 64)\t6\n",
      "  (0, 176)\t4\n",
      "  (0, 178)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 133)\t2\n",
      "  (0, 195)\t1\n",
      "  (0, 68)\t2\n",
      "  (0, 126)\t2\n",
      "  (0, 35)\t7\n",
      "  (0, 103)\t2\n",
      "  (0, 112)\t3\n",
      "  (0, 19)\t3\n",
      "  (0, 183)\t3\n",
      "  (0, 203)\t1\n",
      "  (0, 149)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 85)\t1\n",
      "  (0, 95)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 47)\t1\n",
      "  (0, 138)\t1\n",
      "  (0, 107)\t1\n",
      "  :\t:\n",
      "  (0, 218)\t1\n",
      "  (0, 99)\t1\n",
      "  (0, 196)\t2\n",
      "  (0, 153)\t2\n",
      "  (0, 198)\t1\n",
      "  (0, 169)\t1\n",
      "  (0, 58)\t1\n",
      "  (0, 172)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 119)\t1\n",
      "  (0, 65)\t1\n",
      "  (0, 210)\t1\n",
      "  (0, 106)\t1\n",
      "  (0, 213)\t1\n",
      "  (0, 181)\t1\n",
      "  (0, 158)\t1\n",
      "  (0, 109)\t1\n",
      "  (0, 105)\t1\n",
      "  (0, 98)\t1\n",
      "  (0, 111)\t1\n",
      "  (0, 170)\t1\n",
      "  (0, 205)\t1\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix-Typ:\",type(vecs_count))\n",
    "print(\"Matrix-Dimensionen: \", vecs_count.shape)\n",
    "print(\"\\nEin Dokumentvektor\")\n",
    "print(vecs_count.getrow(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transponieren -> Die Term-Dokument-Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [7 0]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 2]\n",
      " [9 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [6 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 3]\n",
      " [1 0]\n",
      " [3 3]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 3]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 4]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [4 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [0 1]\n",
      " [0 3]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 5]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 4]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 3]\n",
      " [0 5]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "#Transponieren der Matrix\n",
    "inverted_count = vecs_count.transpose()\n",
    "\n",
    "print(\"Index\")\n",
    "#print(inverted_count)\n",
    "#print(\" \")\n",
    "print(inverted_count.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zugriff auf den Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix-Typ: <class 'scipy.sparse._csc.csc_matrix'>\n",
      "Matrix-Dimensionen:  (220, 2)\n",
      "\n",
      "Einige Vorkommenslisten\n",
      "Hochschule\n",
      "  (0, 0)\t3\n",
      "  (0, 1)\t3\n",
      "Anhalt\n",
      "  (0, 0)\t3\n",
      "  (0, 1)\t2\n",
      "modern\n",
      "  (0, 0)\t2\n",
      "=\n",
      "  (0, 0)\t2\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix-Typ:\",type(inverted_count))\n",
    "print(\"Matrix-Dimensionen: \", inverted_count.shape)\n",
    "\n",
    "print(\"\\nEinige Vorkommenslisten\")\n",
    "print(\"Hochschule\")\n",
    "print(inverted_count.getrow(112))\n",
    "print(\"Anhalt\")\n",
    "print(inverted_count.getrow(19))\n",
    "print(\"modern\")\n",
    "print(inverted_count.getrow(153))\n",
    "print(\"=\")\n",
    "print(vecs_count.getcol(153))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}